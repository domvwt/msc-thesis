{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/25 19:05:29 WARN Utils: Your hostname, domvwt-XPS-13-9305 resolves to a loopback address: 127.0.1.1; using 192.168.0.24 instead (on interface wlp164s0)\n",
      "22/06/25 19:05:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/06/25 19:05:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/25 19:05:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/06/25 19:05:30 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "import yaml\n",
    "\n",
    "import graphframes as gf\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Column\n",
    "\n",
    "while not Path(\"data\") in Path(\".\").iterdir():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "plt.style.use(\"seaborn-white\")\n",
    "conf_dict = yaml.safe_load(Path(\"config/conf.yaml\").read_text())\n",
    "\n",
    "checkpoint_dir = str(Path(\"spark-checkpoints\").absolute())\n",
    "graphframes_jar_path = str(\n",
    "    Path(\n",
    "        \".venv/lib/python3.9/site-packages/pyspark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\"\n",
    "    ).absolute()\n",
    ")\n",
    "\n",
    "spark_conf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.jars\", graphframes_jar_path)\n",
    "    .set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    ")\n",
    "\n",
    "sc = SparkContext(conf=spark_conf).getOrCreate()\n",
    "sc.setCheckpointDir(checkpoint_dir)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"8g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.dataprep as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ownership_data_raw': 'data/raw/open-ownership-data.jsonl',\n",
       " 'companies_house_data_raw': 'data/raw/BasicCompanyDataAsOneFile-2022-05-01.csv',\n",
       " 'companies_interim': 'data/interim/companies.parquet',\n",
       " 'relationships_interim': 'data/interim/relationships.parquet',\n",
       " 'persons_interim': 'data/interim/persons.parquet',\n",
       " 'addresses_interim': 'data/interim/addresses.parquet',\n",
       " 'companies_house_interim': 'data/interim/companies-info.parquet',\n",
       " 'companies_processed': 'data/processed/companies.parquet',\n",
       " 'relationships_processed': 'data/processed/relationships.parquet',\n",
       " 'persons_processed': 'data/processed/persons.parquet',\n",
       " 'addresses_processed': 'data/processed/addresses.parquet',\n",
       " 'nodes': 'data/graph/nodes.parquet',\n",
       " 'edges': 'data/graph/edges.parquet',\n",
       " 'connected_components': 'data/graph/connected-components.parquet'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_interim_df = spark.read.parquet(conf_dict[\"persons_interim\"])\n",
    "companies_processed_df = spark.read.parquet(conf_dict[\"companies_processed\"])\n",
    "relationships_processed_df = spark.read.parquet(conf_dict[\"relationships_processed\"])\n",
    "persons_processed_df = spark.read.parquet(conf_dict[\"persons_processed\"])\n",
    "nodes_filtered_df = spark.read.parquet(\"data/graph/component-nodes.parquet\")\n",
    "edges_filtered_df = spark.read.parquet(\"data/graph/component-edges.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------------+----------+------------+--------------------+--------------------------+---------------+---------+-----------------+----+--------------------+--------------------+-----------+------+-------------+--------------------+---------------+-------+--------------------+---------------------+\n",
      "|           addresses| birthDate|dissolutionDate|entityType|foundingDate|         identifiers|incorporatedInJurisdiction|interestedParty|interests|missingInfoReason|name|               names|       nationalities| personType|source|statementDate|         statementID|  statementType|subject|                name|nationalities[0].code|\n",
      "+--------------------+----------+---------------+----------+------------+--------------------+--------------------------+---------------+---------+-----------------+----+--------------------+--------------------+-----------+------+-------------+--------------------+---------------+-------+--------------------+---------------------+\n",
      "|[{319, Chartridge...|1958-02-01|           null|      null|        null|[{/company/091254...|                      null|           null|     null|             null|null|[{Hiten Jackis Da...|[{GB, United King...|knownPerson|  null|         null|openownership-reg...|personStatement|   null|  Hiten Jackis Dayal|                   GB|\n",
      "|[{151, Queensway,...|1982-03-01|           null|      null|        null|[{/company/085472...|                      null|           null|     null|             null|null|[{Anna Koziol, in...|      [{PL, Poland}]|knownPerson|  null|         null|openownership-reg...|personStatement|   null|         Anna Koziol|                   PL|\n",
      "|[{A3 Broomsleigh ...|1973-11-01|           null|      null|        null|[{/company/107609...|                      null|           null|     null|             null|null|[{Samantha Jane R...|[{GB, United King...|knownPerson|  null|         null|openownership-reg...|personStatement|   null|Samantha Jane Rod...|                   GB|\n",
      "|[{3 Pownall Place...|1961-08-01|           null|      null|        null|[{/company/094784...|                      null|           null|     null|             null|null|[{Christopher Fen...|[{GB, United King...|knownPerson|  null|         null|openownership-reg...|personStatement|   null|  Christopher Fensom|                   GB|\n",
      "|[{38, Church Mead...|1960-02-01|           null|      null|        null|[{/company/093912...|                      null|           null|     null|             null|null|[{Mathews Thomas,...|[{GB, United King...|knownPerson|  null|         null|openownership-reg...|personStatement|   null|      Mathews Thomas|                   GB|\n",
      "+--------------------+----------+---------------+----------+------------+--------------------+--------------------------+---------------+---------+-----------------+----+--------------------+--------------------+-----------+------+-------------+--------------------+---------------+-------+--------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of src.dataprep failed: Traceback (most recent call last):\n",
      "  File \"/home/domvwt/projects/msc-thesis/.venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 257, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/domvwt/projects/msc-thesis/.venv/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 455, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/domvwt/.pyenv/versions/3.9.12/lib/python3.9/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 613, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 846, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 983, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 913, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"/home/domvwt/projects/msc-thesis/src/dataprep.py\", line 192\n",
      "    F.col(\"names\").getItem(0).getItem(\"fullName\").alias(\"name\"),\n",
      "    ^\n",
      "SyntaxError: invalid syntax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "dp.process_persons(persons_interim_df).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node count: 205,984\n",
      "Edge count: 204,644\n"
     ]
    }
   ],
   "source": [
    "print(f\"Node count: {nodes_filtered_df.count():,}\")\n",
    "print(f\"Edge count: {edges_filtered_df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|isCompany| count|\n",
      "+---------+------+\n",
      "|     true|171748|\n",
      "|    false| 34236|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes_filtered_df.groupBy(\"isCompany\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap as tw\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "def sanitise_string(column: str) -> Column:\n",
    "    \"\"\"Replace single quotes with double for neo4j import compatibility.\"\"\"\n",
    "    return F.regexp_replace(\n",
    "        F.regexp_replace(F.col(column), pattern='\"', replacement=\"'\"),\n",
    "        pattern=\"'\",\n",
    "        replacement=\"''\",\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_to_csv(df: DataFrame, output_name: str) -> None:\n",
    "    csv_path = f\"data/neo4j/{output_name}.csv\"\n",
    "    parquet_csv_path = f\"{csv_path}.parquet\"\n",
    "    string_cols_sanitised = [\n",
    "        sanitise_string(col[0]).alias(col[0]) if col[1] == \"string\" else col[0]\n",
    "        for col in df.dtypes\n",
    "    ]\n",
    "    sanitised_df = df.select(*string_cols_sanitised)\n",
    "    sanitised_df.coalesce(1).write.csv(\n",
    "        parquet_csv_path, header=False, escape=\"\", mode=\"overwrite\"\n",
    "    )\n",
    "    csv_txt_path = next(Path.cwd().glob(f\"{parquet_csv_path}/part-00000*.csv\"))\n",
    "    Path(csv_txt_path).rename(csv_path)\n",
    "\n",
    "\n",
    "def make_neo4j_statement(df: DataFrame, name: str, entity_type: str) -> None:\n",
    "    acc = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        acc.append(f\"{col}: row.{col}\")\n",
    "\n",
    "    statement = f\"\"\"\\\n",
    "        :auto USING PERIODIC COMMIT LOAD CSV WITH HEADERS FROM \"file:///{name}.csv\" AS row\n",
    "        CREATE (:{entity_type.capitalize()} \"\"\"\n",
    "    statement += \"{\" + \", \".join(acc) + \"});\"\n",
    "    statement = tw.dedent(statement)\n",
    "    print(statement)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d0007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_graph_features_to_entities(\n",
    "    entity_df: DataFrame, nodes_df: DataFrame\n",
    ") -> DataFrame:\n",
    "    nodes_df = nodes_df.drop(\"isCompany\").withColumnRenamed(\"id\", \"statementID\")\n",
    "    return entity_df.join(nodes_df, on=[\"statementID\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------------+--------+---------+-------------+--------+\n",
      "|                  id|isCompany|   component|inDegree|outDegree|triangleCount|pagerank|\n",
      "+--------------------+---------+------------+--------+---------+-------------+--------+\n",
      "| 1034841021205811543|    false| 34359784699|       0|        1|            0|       0|\n",
      "|10441516196839861294|    false| 68719539121|       0|       13|            0|       0|\n",
      "|10479431246556618495|    false|429496771468|       0|       11|            0|       0|\n",
      "|10519184071906386747|    false| 34359785157|       0|        2|            0|       0|\n",
      "|10599056856969872098|    false|566935730925|       0|        3|            0|       0|\n",
      "|10611275659210887798|    false| 51539614510|       0|        1|            0|       0|\n",
      "|10638019556212718020|    false|214748408114|       0|        3|            0|       0|\n",
      "|10946430016329891743|    false|  8589971433|       0|        1|            0|       0|\n",
      "|11019066695878997252|    false|403726944986|       0|        1|            0|       0|\n",
      "|11020645384706326859|    false|266287986261|       0|        1|            0|       0|\n",
      "|11285262454743251626|    false| 42949728106|       0|        1|            0|       0|\n",
      "|11305924287524363468|    false| 42949689903|       0|        5|            0|       0|\n",
      "|11406415553113590643|    false|489626301175|       0|        1|            0|       0|\n",
      "| 1153544239461371268|    false| 34359752052|       0|       11|            0|       0|\n",
      "|  118488738263553428|    false|360777285542|       0|        3|            0|       0|\n",
      "| 1187892489332168324|    false|489626289660|       0|        8|            0|       0|\n",
      "|11954611066936480569|    false|154618878708|       0|        1|            0|       0|\n",
      "|11957142975153869664|    false|266288002958|       0|        1|            0|       0|\n",
      "|11985429964441770450|    false| 34359758033|       0|       10|            0|       0|\n",
      "|12007081987552952522|    false|146028950762|       0|        1|            0|       0|\n",
      "+--------------------+---------+------------+--------+---------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes_filtered_df.filter(~F.col(\"isCompany\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_filtered_df = join_graph_features_to_entities(\n",
    "    companies_processed_df, nodes_filtered_df\n",
    ")\n",
    "persons_filtered_df = join_graph_features_to_entities(\n",
    "    persons_processed_df, nodes_filtered_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Connect company information back to nodes df to make companies nodes df\n",
    "- Connect person information back to nodes df to make persons nodes df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "convert_to_csv(companies_filtered_df, \"companies\")\n",
    "convert_to_csv(persons_filtered_df, \"persons\")\n",
    "convert_to_csv(edges_filtered_df, \"relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":auto USING PERIODIC COMMIT LOAD CSV WITH HEADERS FROM \"file:///companies.csv\" AS row\n",
      "CREATE (:Company {statementID: row.statementID, name: row.name, foundingDate: row.foundingDate, dissolutionDate: row.dissolutionDate, countryCode: row.countryCode, companiesHouseID: row.companiesHouseID, openCorporatesID: row.openCorporatesID, openOwnershipRegisterID: row.openOwnershipRegisterID, CompanyCategory: row.CompanyCategory, CompanyStatus: row.CompanyStatus, Accounts_AccountCategory: row.Accounts_AccountCategory, SICCode_SicText_1: row.SICCode_SicText_1, component: row.component, inDegree: row.inDegree, outDegree: row.outDegree, triangleCount: row.triangleCount, pagerank: row.pagerank});\n",
      "\n",
      ":auto USING PERIODIC COMMIT LOAD CSV WITH HEADERS FROM \"file:///persons.csv\" AS row\n",
      "CREATE (:Person {statementID: row.statementID, birthDate: row.birthDate, nationality: row.nationality, component: row.component, inDegree: row.inDegree, outDegree: row.outDegree, triangleCount: row.triangleCount, pagerank: row.pagerank});\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make_neo4j_statement(companies_filtered_df, \"companies\", \"company\")\n",
    "make_neo4j_statement(persons_filtered_df, \"persons\", \"person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- statementID: string (nullable = true)\n",
      " |-- birthDate: string (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- component: long (nullable = true)\n",
      " |-- inDegree: integer (nullable = true)\n",
      " |-- outDegree: integer (nullable = true)\n",
      " |-- triangleCount: long (nullable = true)\n",
      " |-- pagerank: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persons_filtered_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- src: string (nullable = true)\n",
      " |-- interestedPartyIsPerson: boolean (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      " |-- minimumShare: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_filtered_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cypher\n",
    ":auto USING PERIODIC COMMIT LOAD CSV WITH HEADERS FROM \"file:///relationships.csv\" AS row\n",
    "MATCH\n",
    "  (a:Person),\n",
    "  (b:Company)\n",
    "WHERE a.statementID = row.interestedPartyStatementID AND b.statementID = row.subjectStatementID\n",
    "CREATE (a)-[r:Ownership {minimumShare: row.minimumShare}]->(b);\n",
    "\n",
    ":auto USING PERIODIC COMMIT LOAD CSV WITH HEADERS FROM \"file:///relationships.csv\" AS row\n",
    "MATCH\n",
    "  (a:Company),\n",
    "  (b:Company)\n",
    "WHERE a.statementID = row.interestedPartyStatementID AND b.statementID = row.subjectStatementID\n",
    "CREATE (a)-[r:Ownership {minimumShare: row.minimumShare}]->(b)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9b131bfea46adc0e6841e7be18b140852cf163d67d3b9948cbb78fda58292a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
