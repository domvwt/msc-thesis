{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.dropbox.com/s/yh4grpeks87ugr2/DBLP_processed.zip?dl=1\n",
      "Extracting ../data/datasets/DBLP/raw/DBLP_processed.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mauthor\u001b[0m={\n",
      "    x=[4057, 334],\n",
      "    y=[4057],\n",
      "    train_mask=[4057],\n",
      "    val_mask=[4057],\n",
      "    test_mask=[4057]\n",
      "  },\n",
      "  \u001b[1mpaper\u001b[0m={ x=[14328, 4231] },\n",
      "  \u001b[1mterm\u001b[0m={ x=[7723, 50] },\n",
      "  \u001b[1mconference\u001b[0m={ num_nodes=20 },\n",
      "  \u001b[1m(author, to, paper)\u001b[0m={ edge_index=[2, 19645] },\n",
      "  \u001b[1m(paper, to, author)\u001b[0m={ edge_index=[2, 19645] },\n",
      "  \u001b[1m(paper, to, term)\u001b[0m={ edge_index=[2, 85810] },\n",
      "  \u001b[1m(paper, to, conference)\u001b[0m={ edge_index=[2, 14328] },\n",
      "  \u001b[1m(term, to, paper)\u001b[0m={ edge_index=[2, 85810] },\n",
      "  \u001b[1m(conference, to, paper)\u001b[0m={ edge_index=[2, 14328] }\n",
      ")\n",
      "Epoch: 001, Loss: 1.3903, Train: 0.4100, Val: 0.3075, Test: 0.3491\n",
      "Epoch: 002, Loss: 1.3038, Train: 0.5925, Val: 0.4400, Test: 0.4851\n",
      "Epoch: 003, Loss: 1.2089, Train: 0.7450, Val: 0.5325, Test: 0.6045\n",
      "Epoch: 004, Loss: 1.0839, Train: 0.8025, Val: 0.6050, Test: 0.6721\n",
      "Epoch: 005, Loss: 0.9363, Train: 0.8925, Val: 0.6550, Test: 0.7025\n",
      "Epoch: 006, Loss: 0.7749, Train: 0.9200, Val: 0.6750, Test: 0.7295\n",
      "Epoch: 007, Loss: 0.6137, Train: 0.9375, Val: 0.7000, Test: 0.7513\n",
      "Epoch: 008, Loss: 0.4651, Train: 0.9700, Val: 0.7250, Test: 0.7918\n",
      "Epoch: 009, Loss: 0.3345, Train: 0.9850, Val: 0.7525, Test: 0.8109\n",
      "Epoch: 010, Loss: 0.2249, Train: 0.9900, Val: 0.7825, Test: 0.8232\n",
      "Epoch: 011, Loss: 0.1420, Train: 0.9950, Val: 0.7900, Test: 0.8317\n",
      "Epoch: 012, Loss: 0.0857, Train: 0.9975, Val: 0.7950, Test: 0.8314\n",
      "Epoch: 013, Loss: 0.0490, Train: 0.9975, Val: 0.7925, Test: 0.8308\n",
      "Epoch: 014, Loss: 0.0270, Train: 1.0000, Val: 0.7950, Test: 0.8274\n",
      "Epoch: 015, Loss: 0.0151, Train: 1.0000, Val: 0.8050, Test: 0.8274\n",
      "Epoch: 016, Loss: 0.0087, Train: 1.0000, Val: 0.8050, Test: 0.8238\n",
      "Epoch: 017, Loss: 0.0053, Train: 1.0000, Val: 0.8050, Test: 0.8262\n",
      "Epoch: 018, Loss: 0.0035, Train: 1.0000, Val: 0.8000, Test: 0.8256\n",
      "Epoch: 019, Loss: 0.0025, Train: 1.0000, Val: 0.8000, Test: 0.8247\n",
      "Epoch: 020, Loss: 0.0020, Train: 1.0000, Val: 0.8050, Test: 0.8241\n",
      "Epoch: 021, Loss: 0.0017, Train: 1.0000, Val: 0.8075, Test: 0.8253\n",
      "Epoch: 022, Loss: 0.0016, Train: 1.0000, Val: 0.8025, Test: 0.8250\n",
      "Epoch: 023, Loss: 0.0016, Train: 1.0000, Val: 0.8050, Test: 0.8256\n",
      "Epoch: 024, Loss: 0.0016, Train: 1.0000, Val: 0.8025, Test: 0.8241\n",
      "Epoch: 025, Loss: 0.0017, Train: 1.0000, Val: 0.7975, Test: 0.8222\n",
      "Epoch: 026, Loss: 0.0018, Train: 1.0000, Val: 0.7975, Test: 0.8213\n",
      "Epoch: 027, Loss: 0.0019, Train: 1.0000, Val: 0.8000, Test: 0.8213\n",
      "Epoch: 028, Loss: 0.0021, Train: 1.0000, Val: 0.8000, Test: 0.8182\n",
      "Epoch: 029, Loss: 0.0024, Train: 1.0000, Val: 0.8025, Test: 0.8176\n",
      "Epoch: 030, Loss: 0.0026, Train: 1.0000, Val: 0.8050, Test: 0.8173\n",
      "Epoch: 031, Loss: 0.0029, Train: 1.0000, Val: 0.8025, Test: 0.8167\n",
      "Epoch: 032, Loss: 0.0031, Train: 1.0000, Val: 0.8050, Test: 0.8185\n",
      "Epoch: 033, Loss: 0.0033, Train: 1.0000, Val: 0.8050, Test: 0.8179\n",
      "Epoch: 034, Loss: 0.0035, Train: 1.0000, Val: 0.8075, Test: 0.8182\n",
      "Epoch: 035, Loss: 0.0037, Train: 1.0000, Val: 0.8075, Test: 0.8192\n",
      "Epoch: 036, Loss: 0.0038, Train: 1.0000, Val: 0.8100, Test: 0.8207\n",
      "Epoch: 037, Loss: 0.0040, Train: 1.0000, Val: 0.8125, Test: 0.8201\n",
      "Epoch: 038, Loss: 0.0041, Train: 1.0000, Val: 0.8125, Test: 0.8179\n",
      "Epoch: 039, Loss: 0.0043, Train: 1.0000, Val: 0.8125, Test: 0.8195\n",
      "Epoch: 040, Loss: 0.0044, Train: 1.0000, Val: 0.8125, Test: 0.8189\n",
      "Epoch: 041, Loss: 0.0045, Train: 1.0000, Val: 0.8150, Test: 0.8176\n",
      "Epoch: 042, Loss: 0.0046, Train: 1.0000, Val: 0.8150, Test: 0.8161\n",
      "Epoch: 043, Loss: 0.0046, Train: 1.0000, Val: 0.8075, Test: 0.8170\n",
      "Epoch: 044, Loss: 0.0046, Train: 1.0000, Val: 0.8100, Test: 0.8158\n",
      "Epoch: 045, Loss: 0.0046, Train: 1.0000, Val: 0.8075, Test: 0.8161\n",
      "Epoch: 046, Loss: 0.0046, Train: 1.0000, Val: 0.8125, Test: 0.8161\n",
      "Epoch: 047, Loss: 0.0046, Train: 1.0000, Val: 0.8100, Test: 0.8142\n",
      "Epoch: 048, Loss: 0.0046, Train: 1.0000, Val: 0.8100, Test: 0.8149\n",
      "Epoch: 049, Loss: 0.0046, Train: 1.0000, Val: 0.8075, Test: 0.8146\n",
      "Epoch: 050, Loss: 0.0046, Train: 1.0000, Val: 0.8025, Test: 0.8149\n",
      "Epoch: 051, Loss: 0.0046, Train: 1.0000, Val: 0.8025, Test: 0.8146\n",
      "Epoch: 052, Loss: 0.0046, Train: 1.0000, Val: 0.7975, Test: 0.8133\n",
      "Epoch: 053, Loss: 0.0045, Train: 1.0000, Val: 0.7975, Test: 0.8133\n",
      "Epoch: 054, Loss: 0.0045, Train: 1.0000, Val: 0.7975, Test: 0.8133\n",
      "Epoch: 055, Loss: 0.0044, Train: 1.0000, Val: 0.7975, Test: 0.8127\n",
      "Epoch: 056, Loss: 0.0044, Train: 1.0000, Val: 0.7975, Test: 0.8112\n",
      "Epoch: 057, Loss: 0.0044, Train: 1.0000, Val: 0.7950, Test: 0.8118\n",
      "Epoch: 058, Loss: 0.0043, Train: 1.0000, Val: 0.7950, Test: 0.8112\n",
      "Epoch: 059, Loss: 0.0043, Train: 1.0000, Val: 0.7925, Test: 0.8112\n",
      "Epoch: 060, Loss: 0.0043, Train: 1.0000, Val: 0.7925, Test: 0.8130\n",
      "Epoch: 061, Loss: 0.0042, Train: 1.0000, Val: 0.7950, Test: 0.8136\n",
      "Epoch: 062, Loss: 0.0042, Train: 1.0000, Val: 0.7975, Test: 0.8130\n",
      "Epoch: 063, Loss: 0.0042, Train: 1.0000, Val: 0.8000, Test: 0.8118\n",
      "Epoch: 064, Loss: 0.0041, Train: 1.0000, Val: 0.8025, Test: 0.8112\n",
      "Epoch: 065, Loss: 0.0041, Train: 1.0000, Val: 0.8050, Test: 0.8115\n",
      "Epoch: 066, Loss: 0.0040, Train: 1.0000, Val: 0.8050, Test: 0.8115\n",
      "Epoch: 067, Loss: 0.0040, Train: 1.0000, Val: 0.8025, Test: 0.8118\n",
      "Epoch: 068, Loss: 0.0039, Train: 1.0000, Val: 0.8000, Test: 0.8115\n",
      "Epoch: 069, Loss: 0.0039, Train: 1.0000, Val: 0.8000, Test: 0.8109\n",
      "Epoch: 070, Loss: 0.0039, Train: 1.0000, Val: 0.8000, Test: 0.8106\n",
      "Epoch: 071, Loss: 0.0038, Train: 1.0000, Val: 0.8025, Test: 0.8115\n",
      "Epoch: 072, Loss: 0.0038, Train: 1.0000, Val: 0.8025, Test: 0.8121\n",
      "Epoch: 073, Loss: 0.0037, Train: 1.0000, Val: 0.8025, Test: 0.8121\n",
      "Epoch: 074, Loss: 0.0037, Train: 1.0000, Val: 0.8000, Test: 0.8121\n",
      "Epoch: 075, Loss: 0.0037, Train: 1.0000, Val: 0.8000, Test: 0.8115\n",
      "Epoch: 076, Loss: 0.0036, Train: 1.0000, Val: 0.8000, Test: 0.8118\n",
      "Epoch: 077, Loss: 0.0036, Train: 1.0000, Val: 0.8000, Test: 0.8121\n",
      "Epoch: 078, Loss: 0.0036, Train: 1.0000, Val: 0.7975, Test: 0.8127\n",
      "Epoch: 079, Loss: 0.0035, Train: 1.0000, Val: 0.8000, Test: 0.8121\n",
      "Epoch: 080, Loss: 0.0035, Train: 1.0000, Val: 0.8000, Test: 0.8121\n",
      "Epoch: 081, Loss: 0.0035, Train: 1.0000, Val: 0.8000, Test: 0.8112\n",
      "Epoch: 082, Loss: 0.0034, Train: 1.0000, Val: 0.8000, Test: 0.8121\n",
      "Epoch: 083, Loss: 0.0034, Train: 1.0000, Val: 0.8000, Test: 0.8124\n",
      "Epoch: 084, Loss: 0.0034, Train: 1.0000, Val: 0.8025, Test: 0.8124\n",
      "Epoch: 085, Loss: 0.0034, Train: 1.0000, Val: 0.8025, Test: 0.8124\n",
      "Epoch: 086, Loss: 0.0033, Train: 1.0000, Val: 0.8050, Test: 0.8124\n",
      "Epoch: 087, Loss: 0.0033, Train: 1.0000, Val: 0.8050, Test: 0.8127\n",
      "Epoch: 088, Loss: 0.0033, Train: 1.0000, Val: 0.8050, Test: 0.8127\n",
      "Epoch: 089, Loss: 0.0033, Train: 1.0000, Val: 0.8050, Test: 0.8124\n",
      "Epoch: 090, Loss: 0.0032, Train: 1.0000, Val: 0.8050, Test: 0.8121\n",
      "Epoch: 091, Loss: 0.0032, Train: 1.0000, Val: 0.8050, Test: 0.8121\n",
      "Epoch: 092, Loss: 0.0032, Train: 1.0000, Val: 0.8050, Test: 0.8124\n",
      "Epoch: 093, Loss: 0.0032, Train: 1.0000, Val: 0.8050, Test: 0.8124\n",
      "Epoch: 094, Loss: 0.0032, Train: 1.0000, Val: 0.8050, Test: 0.8127\n",
      "Epoch: 095, Loss: 0.0032, Train: 1.0000, Val: 0.8050, Test: 0.8130\n",
      "Epoch: 096, Loss: 0.0031, Train: 1.0000, Val: 0.8050, Test: 0.8130\n",
      "Epoch: 097, Loss: 0.0031, Train: 1.0000, Val: 0.8050, Test: 0.8130\n",
      "Epoch: 098, Loss: 0.0031, Train: 1.0000, Val: 0.8025, Test: 0.8130\n",
      "Epoch: 099, Loss: 0.0031, Train: 1.0000, Val: 0.8050, Test: 0.8127\n",
      "Epoch: 100, Loss: 0.0031, Train: 1.0000, Val: 0.8050, Test: 0.8130\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import DBLP\n",
    "from torch_geometric.nn import HeteroConv, Linear, SAGEConv\n",
    "\n",
    "# TODO: regularisation like https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch\n",
    "\n",
    "\n",
    "# path = osp.join(osp.dirname(osp.realpath(__file__)), '../../data/DBLP')\n",
    "path = Path(\"../data/datasets/DBLP\")\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "dataset = DBLP(str(path))\n",
    "data = dataset[0]\n",
    "print(data)\n",
    "\n",
    "# We initialize conference node features with a single feature.\n",
    "data[\"conference\"].x = torch.ones(data[\"conference\"].num_nodes, 1)\n",
    "\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv(\n",
    "                {\n",
    "                    edge_type: SAGEConv((-1, -1), hidden_channels)\n",
    "                    for edge_type in metadata[1]\n",
    "                }\n",
    "            )\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: F.leaky_relu(x) for key, x in x_dict.items()}\n",
    "        return self.lin(x_dict[\"author\"])\n",
    "\n",
    "\n",
    "model = HeteroGNN(data.metadata(), hidden_channels=64, out_channels=4, num_layers=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data, model = data.to(device), model.to(device)\n",
    "\n",
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "    mask = data[\"author\"].train_mask\n",
    "    loss = F.cross_entropy(out[mask], data[\"author\"].y[mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict).argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for split in [\"train_mask\", \"val_mask\", \"test_mask\"]:\n",
    "        mask = data[\"author\"][split]\n",
    "        acc = (pred[mask] == data[\"author\"].y[mask]).sum() / mask.sum()\n",
    "        accs.append(float(acc))\n",
    "    return accs\n",
    "\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    print(\n",
    "        f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, \"\n",
    "        f\"Val: {val_acc:.4f}, Test: {test_acc:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9b131bfea46adc0e6841e7be18b140852cf163d67d3b9948cbb78fda58292a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
