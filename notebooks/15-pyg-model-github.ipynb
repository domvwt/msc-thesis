{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5a71bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import to_hetero\n",
    "\n",
    "\n",
    "from mscproject.metrics import EvalMetrics, EvalMetricsTuple\n",
    "from mscproject import models\n",
    "from mscproject.datasets import CompanyBeneficialOwners\n",
    "\n",
    "# TODO: regularisation like https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch\n",
    "# TODO: follow this example https://github.com/pyg-team/pytorch_geometric/issues/3958\n",
    "\n",
    "while not Path(\"data\") in Path(\".\").iterdir():\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e37eb8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_dict = yaml.safe_load(Path(\"config/conf.yaml\").read_text())\n",
    "dataset_path = \"data/pyg/\"\n",
    "\n",
    "dataset = CompanyBeneficialOwners(dataset_path, to_undirected=True)\n",
    "\n",
    "input_data = dataset[0]  # type: ignore\n",
    "input_metadata = dataset.metadata()\n",
    "\n",
    "model = models.GAT(\n",
    "    in_channels=-1,\n",
    "    hidden_channels=16,\n",
    "    num_layers=3,\n",
    "    out_channels=1,\n",
    "    jk=\"last\",\n",
    "    # heads=1,\n",
    "    # concat=True,\n",
    "    v2=True,\n",
    "    add_self_loops=False,\n",
    ")\n",
    "\n",
    "model = to_hetero(model, metadata=input_metadata, aggr=\"sum\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset, model = dataset.data.to(device), model.to(device)\n",
    "\n",
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    out = model(dataset.x_dict, dataset.edge_index_dict)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=0.01, weight_decay=5e-4, amsgrad=False\n",
    ")\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=0.05, weight_decay=0)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0, nesterov=False, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1f880675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(input_data.x_dict, input_data.edge_index_dict)\n",
    "\n",
    "    company_train_mask = input_data[\"company\"].train_mask\n",
    "    person_train_mask = input_data[\"person\"].train_mask\n",
    "\n",
    "    companies_out = out[\"company\"][company_train_mask]\n",
    "    persons_out = out[\"person\"][person_train_mask]\n",
    "    out_tensor = torch.cat((companies_out, persons_out), dim=0).float().squeeze()\n",
    "\n",
    "    companies_y = input_data.y_dict[\"company\"][company_train_mask]\n",
    "    persons_y = input_data.y_dict[\"person\"][person_train_mask]\n",
    "\n",
    "    y_tensor = torch.cat((companies_y, persons_y), dim=0).float().squeeze()\n",
    "\n",
    "    # Multiply importance of anomalous data by 10.\n",
    "    importance = (y_tensor * 9) + 1\n",
    "\n",
    "    loss = F.binary_cross_entropy(out_tensor, y_tensor, weight=importance)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7a5267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "320fe37a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "class EvalResult(NamedTuple):\n",
    "    train: EvalMetrics\n",
    "    val: EvalMetrics\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test() -> EvalResult:\n",
    "    model.eval()\n",
    "\n",
    "    prediction_dict = model(input_data.x_dict, input_data.edge_index_dict)\n",
    "\n",
    "    eval_metrics_list = []\n",
    "\n",
    "    for split in [\"train_mask\", \"val_mask\"]:\n",
    "\n",
    "        masks = []\n",
    "        actuals = []\n",
    "        predictions = []\n",
    "\n",
    "        for node_type in [\"company\", \"person\"]:\n",
    "            mask = input_data[node_type][split]\n",
    "            actual = input_data.y_dict[node_type][mask]\n",
    "            prediction = prediction_dict[node_type][mask]\n",
    "\n",
    "            masks.append(mask)\n",
    "            predictions.append(prediction)\n",
    "            actuals.append(actual)\n",
    "\n",
    "        combined_predictions = torch.cat(predictions, dim=0).squeeze()\n",
    "        combined_actuals = torch.cat(actuals, dim=0).squeeze()\n",
    "\n",
    "        eval_metrics_list.append(\n",
    "            EvalMetrics.from_tensors(\n",
    "                combined_predictions, combined_actuals, pos_weight_multiplier=10\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return EvalResult(*eval_metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4f929942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Early Stopping Callback\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.epoch = 0\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.best_epoch = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "\n",
    "        self.epoch += 1\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_loss = val_loss\n",
    "            self.best_epoch = self.epoch\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4158f280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001\n",
      "Train: loss: 1.329, acc: 0.898, prc: 0.107, rec: 0.856, f1: 0.190, auc: 0.527, aprc: 0.108\n",
      "Val: loss: 1.252, acc: 0.912, prc: 0.094, rec: 0.887, f1: 0.171, auc: 0.533, aprc: 0.093\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 002\n",
      "Train: loss: 1.323, acc: 0.898, prc: 0.107, rec: 0.930, f1: 0.191, auc: 0.545, aprc: 0.113\n",
      "Val: loss: 1.250, acc: 0.912, prc: 0.094, rec: 0.952, f1: 0.171, auc: 0.550, aprc: 0.098\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 003\n",
      "Train: loss: 1.311, acc: 0.898, prc: 0.108, rec: 0.920, f1: 0.194, auc: 0.568, aprc: 0.124\n",
      "Val: loss: 1.242, acc: 0.912, prc: 0.094, rec: 0.923, f1: 0.170, auc: 0.559, aprc: 0.103\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 004\n",
      "Train: loss: 1.305, acc: 0.898, prc: 0.111, rec: 0.865, f1: 0.197, auc: 0.582, aprc: 0.138\n",
      "Val: loss: 1.237, acc: 0.912, prc: 0.097, rec: 0.877, f1: 0.174, auc: 0.556, aprc: 0.109\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 005\n",
      "Train: loss: 1.299, acc: 0.898, prc: 0.114, rec: 0.824, f1: 0.200, auc: 0.592, aprc: 0.149\n",
      "Val: loss: 1.233, acc: 0.912, prc: 0.096, rec: 0.812, f1: 0.172, auc: 0.554, aprc: 0.115\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 006\n",
      "Train: loss: 1.292, acc: 0.898, prc: 0.115, rec: 0.822, f1: 0.202, auc: 0.600, aprc: 0.156\n",
      "Val: loss: 1.229, acc: 0.912, prc: 0.097, rec: 0.798, f1: 0.172, auc: 0.556, aprc: 0.119\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 007\n",
      "Train: loss: 1.286, acc: 0.898, prc: 0.115, rec: 0.835, f1: 0.203, auc: 0.607, aprc: 0.159\n",
      "Val: loss: 1.226, acc: 0.912, prc: 0.098, rec: 0.822, f1: 0.175, auc: 0.562, aprc: 0.124\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 008\n",
      "Train: loss: 1.279, acc: 0.898, prc: 0.115, rec: 0.844, f1: 0.203, auc: 0.612, aprc: 0.169\n",
      "Val: loss: 1.223, acc: 0.912, prc: 0.098, rec: 0.818, f1: 0.175, auc: 0.567, aprc: 0.128\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 009\n",
      "Train: loss: 1.273, acc: 0.898, prc: 0.115, rec: 0.848, f1: 0.203, auc: 0.615, aprc: 0.179\n",
      "Val: loss: 1.220, acc: 0.912, prc: 0.099, rec: 0.818, f1: 0.176, auc: 0.572, aprc: 0.132\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 010\n",
      "Train: loss: 1.267, acc: 0.898, prc: 0.117, rec: 0.842, f1: 0.205, auc: 0.619, aprc: 0.187\n",
      "Val: loss: 1.218, acc: 0.912, prc: 0.101, rec: 0.816, f1: 0.179, auc: 0.576, aprc: 0.137\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 011\n",
      "Train: loss: 1.262, acc: 0.898, prc: 0.118, rec: 0.825, f1: 0.207, auc: 0.624, aprc: 0.192\n",
      "Val: loss: 1.214, acc: 0.912, prc: 0.102, rec: 0.798, f1: 0.181, auc: 0.583, aprc: 0.144\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 012\n",
      "Train: loss: 1.256, acc: 0.898, prc: 0.120, rec: 0.797, f1: 0.209, auc: 0.630, aprc: 0.198\n",
      "Val: loss: 1.208, acc: 0.912, prc: 0.103, rec: 0.775, f1: 0.183, auc: 0.593, aprc: 0.152\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 013\n",
      "Train: loss: 1.251, acc: 0.898, prc: 0.123, rec: 0.780, f1: 0.212, auc: 0.636, aprc: 0.203\n",
      "Val: loss: 1.202, acc: 0.912, prc: 0.106, rec: 0.768, f1: 0.186, auc: 0.603, aprc: 0.158\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 014\n",
      "Train: loss: 1.246, acc: 0.898, prc: 0.125, rec: 0.772, f1: 0.215, auc: 0.641, aprc: 0.208\n",
      "Val: loss: 1.197, acc: 0.912, prc: 0.108, rec: 0.756, f1: 0.189, auc: 0.612, aprc: 0.163\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 015\n",
      "Train: loss: 1.240, acc: 0.898, prc: 0.126, rec: 0.769, f1: 0.216, auc: 0.645, aprc: 0.213\n",
      "Val: loss: 1.194, acc: 0.912, prc: 0.109, rec: 0.759, f1: 0.190, auc: 0.615, aprc: 0.165\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 016\n",
      "Train: loss: 1.236, acc: 0.898, prc: 0.126, rec: 0.774, f1: 0.217, auc: 0.647, aprc: 0.218\n",
      "Val: loss: 1.192, acc: 0.912, prc: 0.109, rec: 0.776, f1: 0.192, auc: 0.615, aprc: 0.165\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 017\n",
      "Train: loss: 1.232, acc: 0.898, prc: 0.126, rec: 0.780, f1: 0.217, auc: 0.649, aprc: 0.223\n",
      "Val: loss: 1.192, acc: 0.912, prc: 0.110, rec: 0.782, f1: 0.192, auc: 0.615, aprc: 0.163\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 018\n",
      "Train: loss: 1.229, acc: 0.898, prc: 0.127, rec: 0.778, f1: 0.218, auc: 0.651, aprc: 0.226\n",
      "Val: loss: 1.191, acc: 0.912, prc: 0.110, rec: 0.778, f1: 0.193, auc: 0.617, aprc: 0.160\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 019\n",
      "Train: loss: 1.227, acc: 0.898, prc: 0.128, rec: 0.766, f1: 0.219, auc: 0.653, aprc: 0.228\n",
      "Val: loss: 1.190, acc: 0.912, prc: 0.111, rec: 0.762, f1: 0.193, auc: 0.618, aprc: 0.159\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 020\n",
      "Train: loss: 1.225, acc: 0.898, prc: 0.129, rec: 0.754, f1: 0.220, auc: 0.654, aprc: 0.231\n",
      "Val: loss: 1.188, acc: 0.912, prc: 0.111, rec: 0.739, f1: 0.193, auc: 0.619, aprc: 0.161\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 021\n",
      "Train: loss: 1.222, acc: 0.898, prc: 0.129, rec: 0.758, f1: 0.221, auc: 0.656, aprc: 0.235\n",
      "Val: loss: 1.187, acc: 0.912, prc: 0.112, rec: 0.746, f1: 0.194, auc: 0.620, aprc: 0.163\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 022\n",
      "Train: loss: 1.219, acc: 0.898, prc: 0.129, rec: 0.779, f1: 0.221, auc: 0.657, aprc: 0.239\n",
      "Val: loss: 1.184, acc: 0.912, prc: 0.111, rec: 0.776, f1: 0.194, auc: 0.623, aprc: 0.167\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 023\n",
      "Train: loss: 1.216, acc: 0.898, prc: 0.128, rec: 0.802, f1: 0.220, auc: 0.659, aprc: 0.243\n",
      "Val: loss: 1.181, acc: 0.912, prc: 0.109, rec: 0.793, f1: 0.192, auc: 0.627, aprc: 0.173\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 024\n",
      "Train: loss: 1.213, acc: 0.898, prc: 0.127, rec: 0.815, f1: 0.220, auc: 0.662, aprc: 0.248\n",
      "Val: loss: 1.177, acc: 0.912, prc: 0.110, rec: 0.816, f1: 0.194, auc: 0.630, aprc: 0.176\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 025\n",
      "Train: loss: 1.210, acc: 0.898, prc: 0.127, rec: 0.817, f1: 0.220, auc: 0.664, aprc: 0.252\n",
      "Val: loss: 1.175, acc: 0.912, prc: 0.111, rec: 0.822, f1: 0.195, auc: 0.632, aprc: 0.180\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 026\n",
      "Train: loss: 1.208, acc: 0.898, prc: 0.128, rec: 0.813, f1: 0.221, auc: 0.666, aprc: 0.255\n",
      "Val: loss: 1.173, acc: 0.912, prc: 0.110, rec: 0.811, f1: 0.194, auc: 0.634, aprc: 0.185\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 027\n",
      "Train: loss: 1.206, acc: 0.898, prc: 0.129, rec: 0.808, f1: 0.222, auc: 0.667, aprc: 0.258\n",
      "Val: loss: 1.171, acc: 0.912, prc: 0.110, rec: 0.798, f1: 0.194, auc: 0.635, aprc: 0.188\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 028\n",
      "Train: loss: 1.204, acc: 0.898, prc: 0.130, rec: 0.808, f1: 0.224, auc: 0.668, aprc: 0.261\n",
      "Val: loss: 1.169, acc: 0.912, prc: 0.111, rec: 0.795, f1: 0.194, auc: 0.636, aprc: 0.189\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 029\n",
      "Train: loss: 1.202, acc: 0.898, prc: 0.130, rec: 0.813, f1: 0.224, auc: 0.670, aprc: 0.264\n",
      "Val: loss: 1.168, acc: 0.912, prc: 0.111, rec: 0.806, f1: 0.196, auc: 0.636, aprc: 0.189\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 030\n",
      "Train: loss: 1.199, acc: 0.898, prc: 0.129, rec: 0.821, f1: 0.224, auc: 0.671, aprc: 0.266\n",
      "Val: loss: 1.167, acc: 0.912, prc: 0.111, rec: 0.811, f1: 0.195, auc: 0.638, aprc: 0.191\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 031\n",
      "Train: loss: 1.197, acc: 0.898, prc: 0.130, rec: 0.826, f1: 0.224, auc: 0.672, aprc: 0.269\n",
      "Val: loss: 1.167, acc: 0.912, prc: 0.111, rec: 0.821, f1: 0.195, auc: 0.637, aprc: 0.192\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 032\n",
      "Train: loss: 1.195, acc: 0.898, prc: 0.130, rec: 0.819, f1: 0.225, auc: 0.674, aprc: 0.272\n",
      "Val: loss: 1.165, acc: 0.912, prc: 0.111, rec: 0.809, f1: 0.195, auc: 0.635, aprc: 0.195\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 033\n",
      "Train: loss: 1.193, acc: 0.898, prc: 0.131, rec: 0.811, f1: 0.226, auc: 0.675, aprc: 0.274\n",
      "Val: loss: 1.163, acc: 0.912, prc: 0.111, rec: 0.796, f1: 0.194, auc: 0.635, aprc: 0.197\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 034\n",
      "Train: loss: 1.191, acc: 0.898, prc: 0.132, rec: 0.810, f1: 0.227, auc: 0.676, aprc: 0.277\n",
      "Val: loss: 1.161, acc: 0.912, prc: 0.113, rec: 0.801, f1: 0.197, auc: 0.636, aprc: 0.199\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 035\n",
      "Train: loss: 1.189, acc: 0.898, prc: 0.132, rec: 0.815, f1: 0.227, auc: 0.677, aprc: 0.279\n",
      "Val: loss: 1.161, acc: 0.912, prc: 0.113, rec: 0.806, f1: 0.198, auc: 0.638, aprc: 0.202\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 036\n",
      "Train: loss: 1.187, acc: 0.898, prc: 0.132, rec: 0.811, f1: 0.228, auc: 0.678, aprc: 0.282\n",
      "Val: loss: 1.159, acc: 0.912, prc: 0.114, rec: 0.803, f1: 0.199, auc: 0.641, aprc: 0.206\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 037\n",
      "Train: loss: 1.185, acc: 0.898, prc: 0.134, rec: 0.804, f1: 0.229, auc: 0.679, aprc: 0.285\n",
      "Val: loss: 1.155, acc: 0.912, prc: 0.114, rec: 0.795, f1: 0.199, auc: 0.643, aprc: 0.210\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 038\n",
      "Train: loss: 1.183, acc: 0.898, prc: 0.134, rec: 0.800, f1: 0.229, auc: 0.680, aprc: 0.288\n",
      "Val: loss: 1.153, acc: 0.912, prc: 0.114, rec: 0.793, f1: 0.200, auc: 0.644, aprc: 0.214\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 039\n",
      "Train: loss: 1.181, acc: 0.898, prc: 0.134, rec: 0.802, f1: 0.229, auc: 0.682, aprc: 0.291\n",
      "Val: loss: 1.152, acc: 0.912, prc: 0.114, rec: 0.789, f1: 0.199, auc: 0.644, aprc: 0.218\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 040\n",
      "Train: loss: 1.179, acc: 0.898, prc: 0.134, rec: 0.802, f1: 0.230, auc: 0.683, aprc: 0.294\n",
      "Val: loss: 1.150, acc: 0.912, prc: 0.114, rec: 0.791, f1: 0.199, auc: 0.644, aprc: 0.222\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 041\n",
      "Train: loss: 1.177, acc: 0.898, prc: 0.134, rec: 0.797, f1: 0.230, auc: 0.685, aprc: 0.297\n",
      "Val: loss: 1.148, acc: 0.912, prc: 0.115, rec: 0.785, f1: 0.200, auc: 0.644, aprc: 0.226\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 042\n",
      "Train: loss: 1.174, acc: 0.898, prc: 0.135, rec: 0.795, f1: 0.230, auc: 0.686, aprc: 0.300\n",
      "Val: loss: 1.146, acc: 0.912, prc: 0.116, rec: 0.788, f1: 0.202, auc: 0.645, aprc: 0.230\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 043\n",
      "Train: loss: 1.172, acc: 0.898, prc: 0.134, rec: 0.798, f1: 0.230, auc: 0.687, aprc: 0.302\n",
      "Val: loss: 1.144, acc: 0.912, prc: 0.115, rec: 0.789, f1: 0.201, auc: 0.647, aprc: 0.234\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 044\n",
      "Train: loss: 1.170, acc: 0.898, prc: 0.135, rec: 0.804, f1: 0.231, auc: 0.688, aprc: 0.305\n",
      "Val: loss: 1.142, acc: 0.912, prc: 0.115, rec: 0.798, f1: 0.201, auc: 0.648, aprc: 0.237\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 045\n",
      "Train: loss: 1.168, acc: 0.898, prc: 0.135, rec: 0.799, f1: 0.231, auc: 0.689, aprc: 0.307\n",
      "Val: loss: 1.141, acc: 0.912, prc: 0.115, rec: 0.793, f1: 0.202, auc: 0.649, aprc: 0.238\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 046\n",
      "Train: loss: 1.166, acc: 0.898, prc: 0.135, rec: 0.805, f1: 0.231, auc: 0.690, aprc: 0.310\n",
      "Val: loss: 1.140, acc: 0.912, prc: 0.116, rec: 0.801, f1: 0.202, auc: 0.650, aprc: 0.240\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 047\n",
      "Train: loss: 1.164, acc: 0.898, prc: 0.135, rec: 0.806, f1: 0.231, auc: 0.692, aprc: 0.312\n",
      "Val: loss: 1.139, acc: 0.912, prc: 0.115, rec: 0.796, f1: 0.202, auc: 0.651, aprc: 0.242\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 048\n",
      "Train: loss: 1.162, acc: 0.898, prc: 0.135, rec: 0.803, f1: 0.232, auc: 0.693, aprc: 0.315\n",
      "Val: loss: 1.138, acc: 0.912, prc: 0.117, rec: 0.799, f1: 0.204, auc: 0.651, aprc: 0.243\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 049\n",
      "Train: loss: 1.160, acc: 0.898, prc: 0.136, rec: 0.802, f1: 0.233, auc: 0.694, aprc: 0.317\n",
      "Val: loss: 1.137, acc: 0.912, prc: 0.116, rec: 0.795, f1: 0.203, auc: 0.651, aprc: 0.245\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 050\n",
      "Train: loss: 1.157, acc: 0.898, prc: 0.136, rec: 0.805, f1: 0.233, auc: 0.695, aprc: 0.319\n",
      "Val: loss: 1.137, acc: 0.912, prc: 0.117, rec: 0.796, f1: 0.204, auc: 0.652, aprc: 0.247\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 051\n",
      "Train: loss: 1.155, acc: 0.898, prc: 0.137, rec: 0.801, f1: 0.235, auc: 0.697, aprc: 0.322\n",
      "Val: loss: 1.136, acc: 0.912, prc: 0.116, rec: 0.782, f1: 0.202, auc: 0.653, aprc: 0.250\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 052\n",
      "Train: loss: 1.153, acc: 0.898, prc: 0.138, rec: 0.803, f1: 0.235, auc: 0.698, aprc: 0.324\n",
      "Val: loss: 1.133, acc: 0.912, prc: 0.117, rec: 0.783, f1: 0.204, auc: 0.656, aprc: 0.252\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 053\n",
      "Train: loss: 1.151, acc: 0.898, prc: 0.138, rec: 0.800, f1: 0.236, auc: 0.699, aprc: 0.326\n",
      "Val: loss: 1.135, acc: 0.912, prc: 0.117, rec: 0.785, f1: 0.204, auc: 0.654, aprc: 0.253\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 054\n",
      "Train: loss: 1.150, acc: 0.898, prc: 0.138, rec: 0.807, f1: 0.235, auc: 0.701, aprc: 0.327\n",
      "Val: loss: 1.128, acc: 0.912, prc: 0.119, rec: 0.798, f1: 0.207, auc: 0.661, aprc: 0.255\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 055\n",
      "Train: loss: 1.149, acc: 0.898, prc: 0.139, rec: 0.793, f1: 0.236, auc: 0.701, aprc: 0.328\n",
      "Val: loss: 1.139, acc: 0.912, prc: 0.118, rec: 0.778, f1: 0.205, auc: 0.653, aprc: 0.256\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 056\n",
      "Train: loss: 1.148, acc: 0.898, prc: 0.136, rec: 0.825, f1: 0.234, auc: 0.701, aprc: 0.328\n",
      "Val: loss: 1.124, acc: 0.912, prc: 0.119, rec: 0.830, f1: 0.209, auc: 0.666, aprc: 0.256\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 057\n",
      "Train: loss: 1.145, acc: 0.898, prc: 0.142, rec: 0.764, f1: 0.240, auc: 0.703, aprc: 0.331\n",
      "Val: loss: 1.134, acc: 0.912, prc: 0.117, rec: 0.728, f1: 0.201, auc: 0.657, aprc: 0.259\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 058\n",
      "Train: loss: 1.143, acc: 0.898, prc: 0.136, rec: 0.834, f1: 0.234, auc: 0.705, aprc: 0.334\n",
      "Val: loss: 1.133, acc: 0.912, prc: 0.118, rec: 0.819, f1: 0.206, auc: 0.659, aprc: 0.259\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 059\n",
      "Train: loss: 1.141, acc: 0.898, prc: 0.139, rec: 0.810, f1: 0.237, auc: 0.705, aprc: 0.334\n",
      "Val: loss: 1.124, acc: 0.912, prc: 0.120, rec: 0.795, f1: 0.209, auc: 0.664, aprc: 0.260\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 060\n",
      "Train: loss: 1.140, acc: 0.898, prc: 0.144, rec: 0.760, f1: 0.242, auc: 0.707, aprc: 0.336\n",
      "Val: loss: 1.130, acc: 0.912, prc: 0.118, rec: 0.724, f1: 0.203, auc: 0.657, aprc: 0.260\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 061\n",
      "Train: loss: 1.139, acc: 0.898, prc: 0.137, rec: 0.835, f1: 0.236, auc: 0.708, aprc: 0.337\n",
      "Val: loss: 1.131, acc: 0.912, prc: 0.119, rec: 0.819, f1: 0.207, auc: 0.657, aprc: 0.260\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 062\n",
      "Train: loss: 1.136, acc: 0.898, prc: 0.139, rec: 0.815, f1: 0.238, auc: 0.709, aprc: 0.338\n",
      "Val: loss: 1.123, acc: 0.912, prc: 0.121, rec: 0.802, f1: 0.210, auc: 0.663, aprc: 0.263\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 063\n",
      "Train: loss: 1.136, acc: 0.898, prc: 0.144, rec: 0.763, f1: 0.243, auc: 0.709, aprc: 0.339\n",
      "Val: loss: 1.125, acc: 0.912, prc: 0.120, rec: 0.729, f1: 0.206, auc: 0.659, aprc: 0.263\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 064\n",
      "Train: loss: 1.134, acc: 0.898, prc: 0.139, rec: 0.827, f1: 0.238, auc: 0.711, aprc: 0.341\n",
      "Val: loss: 1.129, acc: 0.912, prc: 0.118, rec: 0.805, f1: 0.205, auc: 0.657, aprc: 0.262\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 065\n",
      "Train: loss: 1.132, acc: 0.898, prc: 0.140, rec: 0.817, f1: 0.239, auc: 0.712, aprc: 0.341\n",
      "Val: loss: 1.125, acc: 0.912, prc: 0.118, rec: 0.793, f1: 0.206, auc: 0.661, aprc: 0.263\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 066\n",
      "Train: loss: 1.131, acc: 0.898, prc: 0.144, rec: 0.776, f1: 0.243, auc: 0.711, aprc: 0.342\n",
      "Val: loss: 1.126, acc: 0.912, prc: 0.118, rec: 0.726, f1: 0.203, auc: 0.660, aprc: 0.263\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 067\n",
      "Train: loss: 1.129, acc: 0.898, prc: 0.139, rec: 0.832, f1: 0.239, auc: 0.714, aprc: 0.343\n",
      "Val: loss: 1.128, acc: 0.912, prc: 0.117, rec: 0.812, f1: 0.205, auc: 0.660, aprc: 0.263\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 068\n",
      "Train: loss: 1.128, acc: 0.898, prc: 0.141, rec: 0.819, f1: 0.240, auc: 0.715, aprc: 0.344\n",
      "Val: loss: 1.126, acc: 0.912, prc: 0.118, rec: 0.795, f1: 0.205, auc: 0.661, aprc: 0.264\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 069\n",
      "Train: loss: 1.127, acc: 0.898, prc: 0.145, rec: 0.773, f1: 0.244, auc: 0.714, aprc: 0.344\n",
      "Val: loss: 1.125, acc: 0.912, prc: 0.119, rec: 0.738, f1: 0.205, auc: 0.662, aprc: 0.265\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 070\n",
      "Train: loss: 1.125, acc: 0.898, prc: 0.141, rec: 0.829, f1: 0.240, auc: 0.716, aprc: 0.346\n",
      "Val: loss: 1.124, acc: 0.912, prc: 0.118, rec: 0.809, f1: 0.206, auc: 0.664, aprc: 0.267\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 071\n",
      "Train: loss: 1.124, acc: 0.898, prc: 0.142, rec: 0.814, f1: 0.241, auc: 0.717, aprc: 0.347\n",
      "Val: loss: 1.126, acc: 0.912, prc: 0.118, rec: 0.789, f1: 0.205, auc: 0.663, aprc: 0.267\n",
      "-------------------------------------------------------------------------------\n",
      "Epoch: 072\n",
      "Train: loss: 1.123, acc: 0.898, prc: 0.146, rec: 0.771, f1: 0.246, auc: 0.716, aprc: 0.346\n",
      "Val: loss: 1.123, acc: 0.912, prc: 0.120, rec: 0.741, f1: 0.206, auc: 0.666, aprc: 0.267\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "Training complete!\n",
      "Best epoch: 62\n",
      "Best validation loss: 1.122889518737793\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(patience=10, verbose=False)\n",
    "\n",
    "metrics_history = []\n",
    "max_epochs = 200\n",
    "\n",
    "while not early_stopping.early_stop and early_stopping.epoch < max_epochs:\n",
    "    loss = train()\n",
    "    eval_metrics = test()\n",
    "    val_loss = eval_metrics.val.loss\n",
    "    stop = early_stopping(val_loss)\n",
    "    metrics_history.append(eval_metrics)\n",
    "    print(f\"Epoch: {early_stopping.epoch:03d}\")\n",
    "    print(f\"Train: {eval_metrics.train}\")\n",
    "    print(f\"Val: {eval_metrics.val}\")\n",
    "    print(\"-\" * 79)\n",
    "\n",
    "print()\n",
    "print(\"-\" * 79)\n",
    "print(\"Training complete!\")\n",
    "print(f\"Best epoch: {early_stopping.best_epoch}\")\n",
    "print(f\"Best validation loss: {early_stopping.best_loss}\")\n",
    "print(\"-\" * 79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4e17bd6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['onehotencoder__CompanyStatus_Active - Proposal to Strike off__processed',\n",
       " 'onehotencoder__CompanyStatus_None__processed',\n",
       " 'onehotencoder__CompanyStatus_infrequent_sklearn__processed',\n",
       " 'onehotencoder__Accounts_AccountCategory_DORMANT__processed',\n",
       " 'onehotencoder__Accounts_AccountCategory_FULL__processed',\n",
       " 'onehotencoder__Accounts_AccountCategory_GROUP__processed',\n",
       " 'onehotencoder__Accounts_AccountCategory_MICRO ENTITY__processed',\n",
       " 'onehotencoder__Accounts_AccountCategory_NO ACCOUNTS FILED__processed',\n",
       " 'onehotencoder__Accounts_AccountCategory_SMALL__processed',\n",
       " 'onehotencoder__Accounts_AccountCategory_TOTAL EXEMPTION FULL__processed',\n",
       " 'onehotencoder__Accounts_AccountCategory_UNAUDITED ABRIDGED__processed',\n",
       " 'onehotencoder__Accounts_AccountCategory_None__processed',\n",
       " 'onehotencoder__Accounts_AccountCategory_infrequent_sklearn__processed',\n",
       " 'onehotencoder__SICCode_SicText_1_41202 - Construction of domestic buildings__processed',\n",
       " 'onehotencoder__SICCode_SicText_1_64209 - Activities of other holding companies n.e.c.__processed',\n",
       " 'onehotencoder__SICCode_SicText_1_68100 - Buying and selling of own real estate__processed',\n",
       " 'onehotencoder__SICCode_SicText_1_68209 - Other letting and operating of own or leased real estate__processed',\n",
       " 'onehotencoder__SICCode_SicText_1_68320 - Management of real estate on a fee or contract basis__processed',\n",
       " 'onehotencoder__SICCode_SicText_1_70100 - Activities of head offices__processed',\n",
       " 'onehotencoder__SICCode_SicText_1_70229 - Management consultancy activities other than financial management__processed',\n",
       " 'onehotencoder__SICCode_SicText_1_74990 - Non-trading company__processed',\n",
       " 'onehotencoder__SICCode_SicText_1_82990 - Other business support service activities n.e.c.__processed',\n",
       " 'onehotencoder__SICCode_SicText_1_96090 - Other service activities n.e.c.__processed',\n",
       " 'onehotencoder__SICCode_SicText_1_99999 - Dormant Company__processed',\n",
       " 'onehotencoder__SICCode_SicText_1_None__processed',\n",
       " 'onehotencoder__SICCode_SicText_1_infrequent_sklearn__processed',\n",
       " 'foundingDate__processed',\n",
       " 'indegree__processed',\n",
       " 'outdegree__processed',\n",
       " 'closeness__processed',\n",
       " 'clustering__processed',\n",
       " 'pagerank__processed']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"company\"].feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4e17bd6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['onehotencoder__nationality_BE__processed',\n",
       " 'onehotencoder__nationality_CA__processed',\n",
       " 'onehotencoder__nationality_CH__processed',\n",
       " 'onehotencoder__nationality_DE__processed',\n",
       " 'onehotencoder__nationality_ES__processed',\n",
       " 'onehotencoder__nationality_GB__processed',\n",
       " 'onehotencoder__nationality_IE__processed',\n",
       " 'onehotencoder__nationality_PH__processed',\n",
       " 'onehotencoder__nationality_PL__processed',\n",
       " 'onehotencoder__nationality_ZA__processed',\n",
       " 'onehotencoder__nationality_None__processed',\n",
       " 'onehotencoder__nationality_infrequent_sklearn__processed',\n",
       " 'birthDate__processed',\n",
       " 'indegree__processed',\n",
       " 'outdegree__processed',\n",
       " 'closeness__processed',\n",
       " 'clustering__processed',\n",
       " 'pagerank__processed']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"person\"].feature_names"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9b131bfea46adc0e6841e7be18b140852cf163d67d3b9948cbb78fda58292a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
