{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /tmp/ipykernel_89837/780168817.py:33 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/domvwt/projects/msc-thesis/notebooks/05-neo4j-processing.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/domvwt/projects/msc-thesis/notebooks/05-neo4j-processing.ipynb#ch0000000?line=20'>21</a>\u001b[0m graphframes_jar_path \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/domvwt/projects/msc-thesis/notebooks/05-neo4j-processing.ipynb#ch0000000?line=21'>22</a>\u001b[0m     Path(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/domvwt/projects/msc-thesis/notebooks/05-neo4j-processing.ipynb#ch0000000?line=22'>23</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m.venv/lib/python3.9/site-packages/pyspark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/domvwt/projects/msc-thesis/notebooks/05-neo4j-processing.ipynb#ch0000000?line=23'>24</a>\u001b[0m     )\u001b[39m.\u001b[39mabsolute()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/domvwt/projects/msc-thesis/notebooks/05-neo4j-processing.ipynb#ch0000000?line=24'>25</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/domvwt/projects/msc-thesis/notebooks/05-neo4j-processing.ipynb#ch0000000?line=26'>27</a>\u001b[0m spark_conf \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/domvwt/projects/msc-thesis/notebooks/05-neo4j-processing.ipynb#ch0000000?line=27'>28</a>\u001b[0m     SparkConf()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/domvwt/projects/msc-thesis/notebooks/05-neo4j-processing.ipynb#ch0000000?line=28'>29</a>\u001b[0m     \u001b[39m.\u001b[39mset(\u001b[39m\"\u001b[39m\u001b[39mspark.jars\u001b[39m\u001b[39m\"\u001b[39m, graphframes_jar_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/domvwt/projects/msc-thesis/notebooks/05-neo4j-processing.ipynb#ch0000000?line=29'>30</a>\u001b[0m     \u001b[39m.\u001b[39mset(\u001b[39m\"\u001b[39m\u001b[39mspark.sql.sources.partitionOverwriteMode\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdynamic\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/domvwt/projects/msc-thesis/notebooks/05-neo4j-processing.ipynb#ch0000000?line=30'>31</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/domvwt/projects/msc-thesis/notebooks/05-neo4j-processing.ipynb#ch0000000?line=32'>33</a>\u001b[0m sc \u001b[39m=\u001b[39m SparkContext(conf\u001b[39m=\u001b[39;49mspark_conf)\u001b[39m.\u001b[39mgetOrCreate()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/domvwt/projects/msc-thesis/notebooks/05-neo4j-processing.ipynb#ch0000000?line=33'>34</a>\u001b[0m sc\u001b[39m.\u001b[39msetCheckpointDir(checkpoint_dir)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/domvwt/projects/msc-thesis/notebooks/05-neo4j-processing.ipynb#ch0000000?line=34'>35</a>\u001b[0m sc\u001b[39m.\u001b[39msetLogLevel(\u001b[39m\"\u001b[39m\u001b[39mERROR\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/msc-thesis/.venv/lib/python3.9/site-packages/pyspark/context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    141\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[1;32m    145\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[0;32m~/projects/msc-thesis/.venv/lib/python3.9/site-packages/pyspark/context.py:342\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    339\u001b[0m     callsite \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_active_spark_context\u001b[39m.\u001b[39m_callsite\n\u001b[1;32m    341\u001b[0m     \u001b[39m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    343\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot run multiple SparkContexts at once; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    344\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mexisting SparkContext(app=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, master=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    345\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m created by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m         \u001b[39m%\u001b[39m (currentAppName, currentMaster,\n\u001b[1;32m    347\u001b[0m             callsite\u001b[39m.\u001b[39mfunction, callsite\u001b[39m.\u001b[39mfile, callsite\u001b[39m.\u001b[39mlinenum))\n\u001b[1;32m    348\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /tmp/ipykernel_89837/780168817.py:33 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "import yaml\n",
    "\n",
    "import graphframes as gf\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Column\n",
    "\n",
    "while not Path(\"data\") in Path(\".\").iterdir():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "plt.style.use(\"seaborn-white\")\n",
    "conf_dict = yaml.safe_load(Path(\"config/conf.yaml\").read_text())\n",
    "\n",
    "checkpoint_dir = str(Path(\"spark-checkpoints\").absolute())\n",
    "graphframes_jar_path = str(\n",
    "    Path(\n",
    "        \".venv/lib/python3.9/site-packages/pyspark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\"\n",
    "    ).absolute()\n",
    ")\n",
    "\n",
    "spark_conf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.jars\", graphframes_jar_path)\n",
    "    .set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    ")\n",
    "\n",
    "sc = SparkContext(conf=spark_conf).getOrCreate()\n",
    "sc.setCheckpointDir(checkpoint_dir)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"8g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_processed_df = spark.read.parquet(conf_dict[\"companies_interim_02\"])\n",
    "relationships_processed_df = spark.read.parquet(conf_dict[\"relationships_interim_02\"])\n",
    "persons_processed_df = spark.read.parquet(conf_dict[\"persons_interim_02\"])\n",
    "nodes_filtered_df = spark.read.parquet(\"data/graph/component-nodes.parquet\")\n",
    "edges_filtered_df = spark.read.parquet(\"data/graph/component-edges.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node count: 205,984\n",
      "Edge count: 204,644\n"
     ]
    }
   ],
   "source": [
    "print(f\"Node count: {nodes_filtered_df.count():,}\")\n",
    "print(f\"Edge count: {edges_filtered_df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|isCompany| count|\n",
      "+---------+------+\n",
      "|     true|171748|\n",
      "|    false| 34236|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes_filtered_df.groupBy(\"isCompany\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap as tw\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "def sanitise_string(column: str) -> Column:\n",
    "    \"\"\"Replace single quotes with double for neo4j import compatibility.\"\"\"\n",
    "    return F.regexp_replace(\n",
    "        F.regexp_replace(F.col(column), pattern='\"', replacement=\"'\"),\n",
    "        pattern=\"'\",\n",
    "        replacement=\"''\",\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_to_csv(df: DataFrame, output_name: str) -> None:\n",
    "    csv_path = f\"data/neo4j/{output_name}.csv\"\n",
    "    parquet_csv_path = f\"{csv_path}.parquet\"\n",
    "    string_cols_sanitised = [\n",
    "        sanitise_string(col[0]).alias(col[0]) if col[1] == \"string\" else col[0]\n",
    "        for col in df.dtypes\n",
    "    ]\n",
    "    sanitised_df = df.select(*string_cols_sanitised)\n",
    "    sanitised_df.coalesce(1).write.csv(\n",
    "        parquet_csv_path, header=False, escape=\"\", mode=\"overwrite\"\n",
    "    )\n",
    "    csv_txt_path = next(Path.cwd().glob(f\"{parquet_csv_path}/part-00000*.csv\"))\n",
    "    Path(csv_txt_path).rename(csv_path)\n",
    "\n",
    "\n",
    "def make_neo4j_statement(df: DataFrame, name: str, entity_type: str) -> None:\n",
    "    acc = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        acc.append(f\"{col}: row.{col}\")\n",
    "\n",
    "    statement = f\"\"\"\\\n",
    "        :auto USING PERIODIC COMMIT LOAD CSV WITH HEADERS FROM \"file:///{name}.csv\" AS row\n",
    "        CREATE (:{entity_type.capitalize()} \"\"\"\n",
    "    statement += \"{\" + \", \".join(acc) + \"});\"\n",
    "    statement = tw.dedent(statement)\n",
    "    print(statement)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d0007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_graph_features_to_entities(\n",
    "    entity_df: DataFrame, nodes_df: DataFrame\n",
    ") -> DataFrame:\n",
    "    nodes_df = nodes_df.drop(\"isCompany\").withColumnRenamed(\"id\", \"statementID\")\n",
    "    return entity_df.join(nodes_df, on=[\"statementID\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------------+--------+---------+-------------+--------+\n",
      "|                  id|isCompany|   component|inDegree|outDegree|triangleCount|pagerank|\n",
      "+--------------------+---------+------------+--------+---------+-------------+--------+\n",
      "| 1034841021205811543|    false| 34359784699|       0|        1|            0|       0|\n",
      "|10441516196839861294|    false| 68719539121|       0|       13|            0|       0|\n",
      "|10479431246556618495|    false|429496771468|       0|       11|            0|       0|\n",
      "|10519184071906386747|    false| 34359785157|       0|        2|            0|       0|\n",
      "|10599056856969872098|    false|566935730925|       0|        3|            0|       0|\n",
      "|10611275659210887798|    false| 51539614510|       0|        1|            0|       0|\n",
      "|10638019556212718020|    false|214748408114|       0|        3|            0|       0|\n",
      "|10946430016329891743|    false|  8589971433|       0|        1|            0|       0|\n",
      "|11019066695878997252|    false|403726944986|       0|        1|            0|       0|\n",
      "|11020645384706326859|    false|266287986261|       0|        1|            0|       0|\n",
      "|11285262454743251626|    false| 42949728106|       0|        1|            0|       0|\n",
      "|11305924287524363468|    false| 42949689903|       0|        5|            0|       0|\n",
      "|11406415553113590643|    false|489626301175|       0|        1|            0|       0|\n",
      "| 1153544239461371268|    false| 34359752052|       0|       11|            0|       0|\n",
      "|  118488738263553428|    false|360777285542|       0|        3|            0|       0|\n",
      "| 1187892489332168324|    false|489626289660|       0|        8|            0|       0|\n",
      "|11954611066936480569|    false|154618878708|       0|        1|            0|       0|\n",
      "|11957142975153869664|    false|266288002958|       0|        1|            0|       0|\n",
      "|11985429964441770450|    false| 34359758033|       0|       10|            0|       0|\n",
      "|12007081987552952522|    false|146028950762|       0|        1|            0|       0|\n",
      "+--------------------+---------+------------+--------+---------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes_filtered_df.filter(~F.col(\"isCompany\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_filtered_df = join_graph_features_to_entities(\n",
    "    companies_processed_df, nodes_filtered_df\n",
    ")\n",
    "persons_filtered_df = join_graph_features_to_entities(\n",
    "    persons_processed_df, nodes_filtered_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- Connect company information back to nodes df to make companies nodes df\n",
    "- Connect person information back to nodes df to make persons nodes df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "convert_to_csv(companies_filtered_df, \"companies\")\n",
    "convert_to_csv(persons_filtered_df, \"persons\")\n",
    "convert_to_csv(edges_filtered_df, \"relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":auto USING PERIODIC COMMIT LOAD CSV WITH HEADERS FROM \"file:///companies.csv\" AS row\n",
      "CREATE (:Company {statementID: row.statementID, name: row.name, foundingDate: row.foundingDate, dissolutionDate: row.dissolutionDate, countryCode: row.countryCode, companiesHouseID: row.companiesHouseID, openCorporatesID: row.openCorporatesID, openOwnershipRegisterID: row.openOwnershipRegisterID, CompanyCategory: row.CompanyCategory, CompanyStatus: row.CompanyStatus, Accounts_AccountCategory: row.Accounts_AccountCategory, SICCode_SicText_1: row.SICCode_SicText_1, component: row.component, inDegree: row.inDegree, outDegree: row.outDegree, triangleCount: row.triangleCount, pagerank: row.pagerank});\n",
      "\n",
      ":auto USING PERIODIC COMMIT LOAD CSV WITH HEADERS FROM \"file:///persons.csv\" AS row\n",
      "CREATE (:Person {statementID: row.statementID, birthDate: row.birthDate, nationality: row.nationality, component: row.component, inDegree: row.inDegree, outDegree: row.outDegree, triangleCount: row.triangleCount, pagerank: row.pagerank});\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make_neo4j_statement(companies_filtered_df, \"companies\", \"company\")\n",
    "make_neo4j_statement(persons_filtered_df, \"persons\", \"person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- statementID: string (nullable = true)\n",
      " |-- birthDate: string (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- component: long (nullable = true)\n",
      " |-- inDegree: integer (nullable = true)\n",
      " |-- outDegree: integer (nullable = true)\n",
      " |-- triangleCount: long (nullable = true)\n",
      " |-- pagerank: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persons_filtered_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- src: string (nullable = true)\n",
      " |-- interestedPartyIsPerson: boolean (nullable = true)\n",
      " |-- dst: string (nullable = true)\n",
      " |-- minimumShare: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_filtered_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cypher\n",
    ":auto USING PERIODIC COMMIT LOAD CSV WITH HEADERS FROM \"file:///relationships.csv\" AS row\n",
    "MATCH\n",
    "  (a:Person),\n",
    "  (b:Company)\n",
    "WHERE a.statementID = row.interestedPartyStatementID AND b.statementID = row.subjectStatementID\n",
    "CREATE (a)-[r:Ownership {minimumShare: row.minimumShare}]->(b);\n",
    "\n",
    ":auto USING PERIODIC COMMIT LOAD CSV WITH HEADERS FROM \"file:///relationships.csv\" AS row\n",
    "MATCH\n",
    "  (a:Company),\n",
    "  (b:Company)\n",
    "WHERE a.statementID = row.interestedPartyStatementID AND b.statementID = row.subjectStatementID\n",
    "CREATE (a)-[r:Ownership {minimumShare: row.minimumShare}]->(b)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9b131bfea46adc0e6841e7be18b140852cf163d67d3b9948cbb78fda58292a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
